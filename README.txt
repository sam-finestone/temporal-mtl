
# An Exploration into Temporal Multi-Task learning - An extension of the MILA paper


Numerous applications of computer vision, such as indoor robotic navigation, autonomous driving, and robotic surgery automation, require multiple task predictions from monocular videos. This makes multi-task learning approaches ideal as they leverage the use of shared feature representations within their networks making them both computationally efficient and prone to mitigating over-fitting on their training data. A few new developments have recently proposed using previous frames of video sequence to help propagate useful contextual information for specific task predictions. Particularly, inspired by the works of a paper called "MILA: Multi-Task Learning from Videos via Efficient Inter-Frame" where they use a SlowFast network (1) coupled with
a lightweight self-attention module to propagate contextual signals across frames. 

In this work we propose an investigation into an extension of their model, where we use two 3D causal convolutions to try to emulate the short and long-range dependencies achieved by their SlowFast network. Furthermore, we propose 3 different fusion methods - a summation method, a 3D convolution layer and a 3D convolutional decoder. The aims of these fusion methods are to offer different ways of propagating task-specific spatial-temporal information across previous frames to aid with the multi-task model predictions. Our approach ensures lower computational operations and a faster inference on new images. However, due to limited computational resources our model was only able to process two previous frames, and this severely affected the modelâ€™s performance. The overall performance was relatively lower than the baselines we had computed. We provide several challenges and limitations of this model and reasons why it did not fully achieve expected capability.
